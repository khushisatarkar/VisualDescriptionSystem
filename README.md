# VisualDescriptionSystem

**VisualDescriptionSystem** is a deep learning project that automatically generates descriptions for images. It combines computer vision and natural language processing to describe the content of an image in natural language.

ğŸš§ **This project is currently in progress.**

## ğŸ“Œ Features

- Extracts image features using a pre-trained CNN model (like InceptionV3 or ResNet).
- Generates captions using LSTM or Transformer-based models.
- Trained on the [Flickr8k](https://www.kaggle.com/datasets/adityajn105/flickr8k) dataset.
- Supports greedy decoding and beam search for caption generation.
- Clean and modular Jupyter Notebook implementation.


## ğŸ§  Model Overview
Encoder: CNN (InceptionV3 or ResNet50) to extract image features.

Decoder: RNN (LSTM) with an embedding layer and a dense output layer.

Training: Uses teacher forcing with padded sequences and categorical crossentropy loss.

## ğŸ“ Project Structure
VisualDescriptionSystem/

â”œâ”€â”€ Images/                 # Dataset images

â”œâ”€â”€ caption_model.h5

â”œâ”€â”€ captions.txt 

â”œâ”€â”€ model.png

â”œâ”€â”€ tokenizer.pkl

â”œâ”€â”€ VisualDescriptionSystem.ipynb  

â””â”€â”€ README.md

## ğŸ§ª To Do
Add support for other datasets (like MS COCO)

Replace RNN decoder with a Transformer

Deploy as a simple web app (Flask or Streamlit)

## ğŸ™‹â€â™€ï¸ Author
Khushi Satarkar

Feel free to reach out or connect with me!
 [LinkedIn](https://www.linkedin.com/in/khushi-satarkar-039056254/) | [Email](mailto:khushisatarkar24@gmail.com)
